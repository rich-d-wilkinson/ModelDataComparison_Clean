{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Demo\n",
    "I'll use this doc to briefly demo the code. The core functionality to fit the GP and evaluate the log-likelihood is just four lines of code (3 lines to train the GP, 1 lines to calculate the likelihood or RMSE etc). Of course we need to load the libraries, data, grids, do plots, print results etc, so that what the rest of the code here is doing.\n",
    "\n",
    "Let's begin by loading the libraries we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import GPy\n",
    "import sys\n",
    "import os\n",
    " \n",
    "module_dir = os.getenv(\"HOME\")+'/Documents/Code/ModelDataComparison_clean/'\n",
    "sys.path.append(module_dir+\"/core/\")\n",
    "from HaversineDist import Exponentialhaversine\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "# If you don't have Basemap installed - comment out this line and use\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import glob as glob\n",
    "from cmocean import cm as cm\n",
    "from Utilities import *\n",
    "from score import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's specify where the data and model runs are located, and load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GCM_dir = module_dir+'data/Model_data/CO2_anom/'\n",
    "obs_dir = module_dir +'data/Observation_data/P3+_SST_anom/'\n",
    "\n",
    "\n",
    "gcm_SSTs = glob.glob(GCM_dir+'t*.txt')\n",
    "gcm_mask = np.genfromtxt(GCM_dir+'mask.txt', dtype='int')\n",
    "file = 'lambda_10.txt'\n",
    "observations = np.genfromtxt(obs_dir+file, skip_header=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll extract the observations into their coordinates (long and lat) which I'll refer to as X, the measurement (Y), and our estimate of the ratio of the variance of the measurement error at each point (NOTE: I was unsure whether the numbers in the data you gave me were supposed to standard deviations or variances - they look like std devs, so I've squared them). Note that all that matters is the relative size of these variances (their ratio). We need to try changing this to find its effect.\n",
    "\n",
    "I've plotted the locations just as a check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need basemap to plot just the points\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'CS' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-318fbbf7d755>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvar_ratios\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_obs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasemap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/pmzrdw/Documents/Code/ModelDataComparison_clean//core/Utilities.py\u001b[0m in \u001b[0;36mplot_map\u001b[0;34m(longgrid, latgrid, vals, X_obs, levels, basemap)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Need basemap to plot just the points'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \"\"\"\n\u001b[1;32m    139\u001b[0m     \u001b[0mmp2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBasemap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cyl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mllcrnrlat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0murcrnrlat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllcrnrlon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m180\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0murcrnrlon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m180\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresolution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'CS' referenced before assignment"
     ]
    }
   ],
   "source": [
    "X_obs = observations[:,0:2]\n",
    "y_obs = observations[:,2].reshape(-1,1)\n",
    "var_ratios = observations[:,3][:,None]**2\n",
    "map = plot_map(X_obs=X_obs, basemap=)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a Gaussian process model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from scaledheteroscedasticgaussian import ScaledHeteroscedasticGaussian\n",
    "from gp_heteroscedastic_ratios import ScaledHeteroscedasticRegression\n",
    "\n",
    "\n",
    "k3 = Exponentialhaversine(2, lengthscale=2000)\n",
    "m3 = ScaledHeteroscedasticRegression(X=X_obs, Y=y_obs, kernel=k3, noise_mult=1., \n",
    "                                     known_variances=var_ratios)\n",
    "m3.optimize_restarts(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the fitting the GP is just three lines of code. We can visualise the model fit by predicting on a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(m3)\n",
    "latsplot = np.arange(-90.0,90.0, 2.5)\n",
    "longsplot = np.arange(-180.0,180.0, 2.5)\n",
    "longgridplot, latgridplot = np.meshgrid(longsplot, latsplot)\n",
    "X_plot=np.column_stack((longgridplot.flatten(), latgridplot.flatten())) # specifies the prediction locations\n",
    "\n",
    "mu3,V3 = m3.predict_noiseless(X_plot)\n",
    "\n",
    "\n",
    "plt.figure(3)\n",
    "map=plot_map(longgridplot, latgridplot, mu3, X_obs)\n",
    "plt.title('Predict mean SST anomaly - spherical GP, heteroscedastic error')\n",
    "\n",
    "plt.figure(4)\n",
    "map=plot_map(longgridplot, latgridplot, np.sqrt(V3), X_obs, levels=np.arange(0,np.sqrt(V3).max()+1,0.25))\n",
    "plt.title('Standard deviation of the prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood calculations\n",
    "\n",
    "Let's now evaluate the 8 GCM simulations we have available. To do this, we need to predict the GCM output at every grid cell, and then evalaute the probability of seeing this value under our GP model fitted to the observational data.\n",
    "\n",
    "We'll begin by loading the GCM runs, and then thinning them onto a coarser grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thinby=2 # this controls the degree of thinning.\n",
    "# thinby=2 takes every second value\n",
    "#Â thinby=1 uses the original GCM grid. This is fine on my laptop (takes about 10 mins), \n",
    "# but my desktop runs out of memory before it completes.\n",
    "\n",
    "count=0\n",
    "gcm_runs = np.zeros((8,27186))\n",
    "gcm_runs_label = gcm_SSTs.copy()\n",
    "\n",
    "for file_name in gcm_SSTs:\n",
    "    file_nm = file_name.split(GCM_dir)[-1]\n",
    "    print(file_nm)\n",
    "    # Read in GCM output.\n",
    "    gcm_runs[count,:] = np.genfromtxt(file_name)\n",
    "    gcm_runs_label[count] = file_nm.split(\".txt\")[0]\n",
    "    count +=1\n",
    "\n",
    "\n",
    "# Create the prediction grid - removing the land coordinates to save computation effort.\n",
    "tmp1, tmp2 = ThinGrid(gcm_runs[0,:], gcm_mask, thinby=thinby) # just to get size\n",
    "gcm_thin = np.zeros((8, tmp2.size))\n",
    "for count in range(8):\n",
    "    X_pred, out = ThinGrid(gcm_runs[count,:], gcm_mask, thinby=thinby)\n",
    "    gcm_thin[count,:] = out.flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've now wrapped the different scoring approaches we considered in the function called 'score'. You have to tell it the method you want to use. The default is to use the GP mean and variance to calculate a loglikelihood, ignoring the covariance. To see the options, type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loglikes_var_only, orderings_var_only, relative_var_only =score(X_pred, gcm_thin, X_obs, y_obs, m3,method = 'gp_var')\n",
    "loglikes_full, orderings_full, relative_full =score(X_pred, gcm_thin, X_obs, y_obs, m3,method = 'gp_full_cov')\n",
    "loglikes_mean, orderings_mean, relative_mean =score(X_pred, gcm_thin, X_obs, y_obs, m3,method = 'gp_mean')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the likelihood calculation is just three lines, but I've wrapped it here. See earlier versions of this notebook, or the code to see the details.\n",
    "\n",
    "We can decode from the file names to the CO2 values to this to see how this ranks the various GCM runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict = {'tdgth': '280 ppm',\n",
    "        'tczyi': '315 ppm',\n",
    "    'tdgtj': '350 ppm',\n",
    "    'tczyj': '375 ppm',\n",
    "    'tdgtg': '405 ppm',\n",
    "    'tczyk': '475 ppm',\n",
    "    'tdgtk': '560 ppm',\n",
    "    'tdgti': '1000 ppm'}\n",
    "\n",
    "orderings = orderings_var_only\n",
    "relative = relative_var_only\n",
    "\n",
    "for ii in range(8):\n",
    "    print(dict[gcm_runs_label[orderings[ii]]] + ':  relative loglike = '+str(relative[orderings[ii]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can compare this with the ranking we obtain using the RMSE as a score function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RMSEs, _, _ =score(X_pred, gcm_thin, X_obs, y_obs, None, method = 'RMSE')\n",
    "print(RMSEs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CO2 = np.zeros(8)\n",
    "for ii in range(8):\n",
    "    CO2[ii] = int(dict[gcm_runs_label[ii]].split(' ppm')[0])\n",
    "    \n",
    "plt.scatter(CO2, -RMSEs)\n",
    "\n",
    "plt.title('-RMSE vs pCO2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(CO2, loglikes_var_only)\n",
    "plt.title('log-likelihood (var only) vs pCO2')\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(CO2, loglikes_full)\n",
    "plt.title('log-likelihood (full cov) vs pCO2')\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(CO2, -loglikes_mean)\n",
    "plt.title('-RMSE using GP mean on grid vs pCO2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "CS = plot_gcm(gcm_runs[orderings[ii],:], gcm_mask, X_obs=X_obs, basemap=True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plot_gcm(gcm_runs[orderings[ii],:], gcm_mask, X_obs=X_obs, basemap=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
